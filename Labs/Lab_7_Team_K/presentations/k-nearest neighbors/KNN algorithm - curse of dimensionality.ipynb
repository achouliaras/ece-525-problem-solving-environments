{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 2: k-nearest neighbors / Curse of Dimensionality\n",
    "---\n",
    "\n",
    "### The k-NN algorithm\n",
    "\n",
    "__Assumption__: Similar Inputs have similar outputs \n",
    "\n",
    "__Classification rule__: For\n",
    "a test input \\$\\\\mathbf{x}\\$, assign the most common label amongst its k most similar training inputs\n",
    "\n",
    "![](note2_0.png)\n",
    "\n",
    "__Figure 1__: A binary classification example with \\$k=3\\$. The green point in the center is the test sample \\$\\\\mathbf{x}\\$. The labels of the 3\n",
    "neighbors are \\$2\\\\times\\$(+1) and \\$1\\\\times\\$(-1) resulting in\n",
    "majority predicting (+1).\n",
    "\n",
    "__Formal (and borderline incomprehensible) definition of k-NN__:\n",
    "\n",
    "* Test point: \\$\\\\mathbf{x}\\$\n",
    "\n",
    "* Denote the set of the \\$k\\$ nearest neighbors of $\\mathbf{x}$ as\n",
    "$S_\\mathbf{x}$. Formally $S_\\mathbf{x}$ is defined as\n",
    "$S_\\mathbf{x}\\subseteq {D}$ s.t. $|S_\\mathbf{x}|=k$ and\n",
    "$\\forall(\\mathbf{x}',y') \\in D \\backslash S_\\mathbf{x}$,\n",
    "\n",
    "$$\\text{dist}(\\mathbf{x},\\mathbf{x}')\\ge \\max_{(\\mathbf{x}'',y'') \\in\n",
    "S_\\mathbf{x}} \\text{dist}(\\mathbf{x},\\mathbf{x}''),$$\n",
    "\n",
    "\n",
    "(i.e. every point in \\$D\\$ but *not* in \\$S\\_\\\\mathbf{x}\\$ is at least as far away\n",
    "from \\$\\\\mathbf{x}\\$ as the furthest point in \\$S\\_\\\\mathbf{x}\\$). We\n",
    "can then define the classifier \\$h()\\$ as a function returning the most\n",
    "common label in \\$S\\_\\\\mathbf{x}\\$:\n",
    "\n",
    "$$h(\\mathbf{x})=\\text{mode}(\\{y'':(\\mathbf{x}'',y'')\\in\n",
    "S_\\mathbf{x}\\}),$$ where \\$\\\\text{mode}(\\\\cdot)\\$ means to select\n",
    "the label of the highest occurrence.\n",
    "\n",
    "(Hint: In case of a draw, a good solution is to return the result of\n",
    "\\$k\\$-NN with smaller \\$k\\$.)\n",
    "\n",
    "**Quiz\\#1:** How does \\$k\\$ affect the classifier? What happens if\n",
    "\\$k=n\\$? What if \\$k =1\\$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking it Down – Pseudo Code of KNN\n",
    "We can implement a KNN model by following the below steps:\n",
    "\\begin{enumerate}\n",
    "\\item Load the data\n",
    "\\item Initialize the value of k\n",
    "\\item For getting the predicted class, iterate from 1 to total number of training data points\n",
    "    \\begin{enumerate}\n",
    "    \\item Calculate the distance between test data and each row of training data. Here we will use Euclidean distance as our distance metric since it’s the most popular method. The other metrics that can be used are Chebyshev, cosine, etc.\n",
    "    \\item Sort the calculated distances in ascending order based on distance values\n",
    "    \\item Get top k rows from the sorted array\n",
    "    \\item Get the most frequent class of these rows\n",
    "    \\item Return the predicted class\n",
    "    \\end{enumerate}\n",
    "\n",
    "\\end{enumerate}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "#### Start of STEP 1\n",
    "# Importing data \n",
    "data = pd.read_csv(\"iris.csv\")\n",
    "#### End of STEP 1\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function which calculates euclidean distance between two data points\n",
    "def euclideanDistance(data1, data2, length):\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += np.square(data1[x] - data2[x])\n",
    "    return np.sqrt(distance)\n",
    "\n",
    "# Defining our KNN model\n",
    "def knn(trainingSet, testInstance, k):\n",
    " \n",
    "    distances = {}\n",
    "    sort = {}\n",
    " \n",
    "    length = testInstance.shape[1]\n",
    "    \n",
    "    #### Start of STEP 3\n",
    "    # Calculating euclidean distance between each row of training data and test data\n",
    "    for x in range(len(trainingSet)):\n",
    "        \n",
    "        #### Start of STEP 3.1\n",
    "        dist = euclideanDistance(testInstance, trainingSet.iloc[x], length)\n",
    "\n",
    "        distances[x] = dist[0]\n",
    "        #### End of STEP 3.1\n",
    " \n",
    "    #### Start of STEP 3.2\n",
    "    # Sorting them on the basis of distance\n",
    "    sorted_d = sorted(distances.items(), key=operator.itemgetter(1))\n",
    "    #### End of STEP 3.2\n",
    " \n",
    "    neighbors = []\n",
    "    \n",
    "    #### Start of STEP 3.3\n",
    "    # Extracting top k neighbors\n",
    "    for x in range(k):\n",
    "        neighbors.append(sorted_d[x][0])\n",
    "    #### End of STEP 3.3\n",
    "    classVotes = {}\n",
    "    \n",
    "    #### Start of STEP 3.4\n",
    "    # Calculating the most freq class in the neighbors\n",
    "    for x in range(len(neighbors)):\n",
    "        response = trainingSet.iloc[neighbors[x]][-1]\n",
    " \n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    #### End of STEP 3.4\n",
    "\n",
    "    #### Start of STEP 3.5\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return(sortedVotes[0][0], neighbors)\n",
    "    #### End of STEP 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dummy testset\n",
    "testSet = [[7.2, 3.6, 5.1, 2.5]]\n",
    "test = pd.DataFrame(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virginica\n"
     ]
    }
   ],
   "source": [
    "#### Start of STEP 2\n",
    "# Setting number of neighbors = 1\n",
    "k = 1\n",
    "#### End of STEP 2\n",
    "# Running KNN model\n",
    "result,neigh = knn(data, test, k)\n",
    "\n",
    "# Predicted class\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[141]\n"
     ]
    }
   ],
   "source": [
    "# Nearest neighbor\n",
    "print(neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virginica\n",
      "[141, 139, 120]\n"
     ]
    }
   ],
   "source": [
    "#Now we will try to alter the k values, and see how the prediction changes.\n",
    "\n",
    "# Setting number of neighbors = 3 \n",
    "k = 3 \n",
    "# Running KNN model \n",
    "result,neigh = knn(data, test, k) \n",
    "# Predicted class \n",
    "print(result) \n",
    "# 3 nearest neighbors\n",
    "print(neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virginica\n",
      "[141, 139, 120, 145, 144]\n"
     ]
    }
   ],
   "source": [
    "# Setting number of neighbors = 5\n",
    "k = 5\n",
    "# Running KNN model \n",
    "result,neigh = knn(data, test, k) \n",
    "# Predicted class \n",
    "print(result) \n",
    "# 5 nearest neighbors\n",
    "print(neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# read in data\n",
    "df = pd.read_csv('iris.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets; set random state to 0 for reproducibility \n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['sepal_length', 'sepal_width', \n",
    "                                                        'petal_length', 'petal_width']],\n",
    "                                                    data['species'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virginica' 'versicolor' 'setosa' 'virginica' 'setosa' 'virginica'\n",
      " 'setosa' 'versicolor' 'versicolor' 'versicolor' 'virginica' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'setosa' 'versicolor' 'versicolor'\n",
      " 'setosa' 'setosa' 'virginica' 'versicolor' 'setosa' 'setosa' 'virginica'\n",
      " 'setosa' 'setosa' 'versicolor' 'versicolor' 'setosa' 'virginica'\n",
      " 'versicolor' 'setosa' 'virginica' 'virginica' 'versicolor' 'setosa'\n",
      " 'virginica']\n",
      "[[ 4 57 46]]\n"
     ]
    }
   ],
   "source": [
    "#Comparing our model with scikit-learn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "# Predicted class\n",
    "y_pred = neigh.predict(X_test)\n",
    "print(neigh.predict(X_test))\n",
    "\n",
    "# 3 nearest neighbors\n",
    "print(neigh.kneighbors(test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.97\n"
     ]
    }
   ],
   "source": [
    "# what is our score?\n",
    "print(\"Test set score: {:.2f}\".format(neigh.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  0,  0],\n",
       "       [ 0, 15,  1],\n",
       "       [ 0,  0,  9]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What distance function should we use?\n",
    "\n",
    "The k-nearest neighbor classifier fundamentally relies on a distance\n",
    "metric. The better that metric reflects label similarity, the better the classified will be. The most common choice is the **Minkowski distance**\n",
    "\n",
    "\\$\\$\\\\text{dist}(\\\\mathbf{x},\\\\mathbf{z})=\\\\left(\\\\sum\\_{r=1}\\^d\n",
    "|x\\_r-z\\_r|\\^p\\\\right)\\^{1/p}.\\$\\$\n",
    "**Quiz\\#2:** This distance definition is pretty general and contains\n",
    "many well-known distances as special cases. Can you identify the\n",
    "following candidates?\n",
    "\n",
    "1.  \\$p = 1\\$:\n",
    "2.  \\$p = 2\\$:\n",
    "3.  \\$p \\\\to \\\\infty\\$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief digression (Bayes optimal classifier)\n",
    "\n",
    "**Example:** Assume (and this is almost never the case) you knew\n",
    "\\$\\\\mathrm{P}(y|\\\\mathbf{x})\\$, then you would simply predict the most\n",
    "likely label. \n",
    "\n",
    "\\$\\$ \\\\text{The Bayes optimal classifier predicts:}\\\\\n",
    "y\\^\\* = h\\_\\\\mathrm{opt}(\\\\mathbf{x}) = \\\\operatorname\\*{argmax}\\_y\n",
    "P(y|\\\\mathbf{x}) \\$\\$ \n",
    "Although the Bayes optimal classifier is as good\n",
    "as it gets, it still can make mistakes. It is always wrong if a sample\n",
    "does not have the most likely label. We can compute the probability of\n",
    "that happening precisely (which is exactly the error rate):\n",
    "\\$\\$\\\\epsilon\\_{BayesOpt}=1-\\\\mathrm{P}(h\\_\\\\mathrm{opt}(\\\\mathbf{x})|y)\n",
    "= 1- \\\\mathrm{P}(y\\^\\*|\\\\mathbf{x})\\$\\$ Assume for example an email\n",
    "\\$\\\\mathbf{x}\\$ can either be classified as spam \\$(+1)\\$ or ham\n",
    "\\$(-1)\\$. For the same email \\$\\\\mathbf{x}\\$ the conditional class\n",
    "probabilities are: \\$\\$ \\\\mathrm{P}(+1| \\\\mathbf{x})=0.8\\\\\\\\\n",
    "\\\\mathrm{P}(-1| \\\\mathbf{x})=0.2\\\\\\\\ \\$\\$ In this case the Bayes optimal\n",
    "classifier would predict the label \\$y\\^\\*=+1\\$ as it is most likely,\n",
    "and its error rate would be \\$\\\\epsilon\\_{BayesOpt}=0.2\\$.\n",
    "\n",
    "Why is the Bayes optimal classifier interesting, if it cannot be used in\n",
    "practice? The reason is that it provides a highly informative lower\n",
    "bound of the error rate. With the same feature representation no\n",
    "classifier can obtain a lower error. We will use this fact to analyze\n",
    "the error rate of the \\$k\\$NN classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Briefer digression: Best constant predictor\n",
    "\n",
    "While we are on the topic, let us also introduce an *upper bound* on the\n",
    "error --- i.e. a classifier that we will (hopefully) always beat. That\n",
    "is the *constant* classifier, which essentially predicts always the same\n",
    "constant independent of any feature vectors. The best constant in\n",
    "classification is the most common label in the training set.\n",
    "Incidentally, that is also what the \\$k\\$-NN classifier becomes if\n",
    "\\$k=n\\$. In regression settings, or more generally, the best constant is\n",
    "the constant that minimizes the loss on the training set (e.g. for the\n",
    "squared loss it is the *average label* in the training set, for the\n",
    "absolute loss the *median label*). The best constant classifier is\n",
    "important for debugging purposes -- you should always be able to show\n",
    "that your classifier performs\n",
    "[significantly](https://en.wikipedia.org/wiki/Statistical_significance)\n",
    "better on the test set than the best constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-NN Convergence Proof\n",
    "\n",
    "**Cover and Hart 1967 : As \\$n \\\\to \\\\infty\\$, the\n",
    "\\$1\\$-NN error is no more than twice the error of the Bayes Optimal\n",
    "classifier.** (Similar guarantees hold for \\$k&gt;1\\$.)\n",
    "\n",
    "\n",
    "  \\$n\\$ small |   \\$n\\\\to\\\\infty\\$ |     \\$n\\$ large \n",
    "  -|-|-\n",
    "  ![](note2_2_1.png) |   ![](note2_2_2.png)|   ![](note2_2_3.png)\n",
    "\n",
    "Let \\$\\\\mathbf{x}\\_\\\\mathrm{NN}\\$ be the nearest neighbor of our test\n",
    "point \\$\\\\mathbf{x}\\_\\\\mathrm{t}\\$. As \\$n \\\\to \\\\infty\\$,\n",
    "\\$\\\\text{dist}(\\\\mathbf{x}\\_\\\\mathrm{NN},\\\\mathbf{x}\\_\\\\mathrm{t}) \\\\to\n",
    "0\\$, i.e. \\$\\\\mathbf{x}\\_\\\\mathrm{NN} \\\\to \\\\mathbf{x}\\_{t}\\$. (This\n",
    "means the nearest neighbor is identical to\n",
    "\\$\\\\mathbf{x}\\_\\\\mathrm{t}\\$.) You return the label of\n",
    "\\$\\\\mathbf{x}\\_\\\\mathrm{NN}\\$. What is the probability that this is not\n",
    "the label of \\$\\\\mathbf{x}\\_\\\\mathrm{t}\\$? (This is the probability of\n",
    "drawing two different label of \\$\\\\mathbf{x}\\$) \\\\begin{multline\\*}\n",
    "\\\\epsilon\\_{NN}=\\\\mathrm{P}(y\\^\\* |\n",
    "\\\\mathbf{x}\\_{t})(1-\\\\mathrm{P}(y\\^\\* | \\\\mathbf{x}\\_{NN})) +\n",
    "\\\\mathrm{P}(y\\^\\* | \\\\mathbf{x}\\_{NN})(1-\\\\mathrm{P}(y\\^\\* |\n",
    "\\\\mathbf{x}\\_{t}))\\\\\\\\ \\\\le (1-\\\\mathrm{P}(y\\^\\* |\n",
    "\\\\mathbf{x}\\_{NN}))+(1-\\\\mathrm{P}(y\\^\\* | \\\\mathbf{x}\\_{t})) =\n",
    "2(1-\\\\mathrm{P}(y\\^\\* | \\\\mathbf{x}\\_{t}) =\n",
    "2\\\\epsilon\\_\\\\mathrm{BayesOpt}, \\\\end{multline\\*} where the inequality\n",
    "follows from \\$\\\\mathrm{P}(y\\^\\* | \\\\mathbf{x}\\_{t})\\\\le 1\\$ and\n",
    "\\$\\\\mathrm{P}(y\\^\\* | \\\\mathbf{x}\\_{NN})\\\\le 1\\$. We also used that\n",
    "\\$\\\\mathrm{P}(y\\^\\* | \\\\mathbf{x}\\_{t})=\\\\mathrm{P}(y\\^\\* |\n",
    "\\\\mathbf{x}\\_{NN})\\$.\n",
    "\n",
    "> ![](spamtree.png)\n",
    "> In the limit case, the test point and its nearest neighbor are\n",
    "> identical. There are exactly two cases when a misclassification can\n",
    "> occur: when the test point and its nearest neighbor have different\n",
    "> labels. The probability of this happening is the probability of the\n",
    "> two red events:\n",
    "> \\$(1\\\\!-\\\\!p(s|\\\\mathbf{x}))p(s|\\\\mathbf{x})+p(s|\\\\mathbf{x})(1\\\\!-\\\\!p(s|\\\\mathbf{x}))=2p(s|\\\\mathbf{x})(1-p(s|\\\\mathbf{x}))\\$\n",
    "\n",
    "Good news: As \\$n \\\\to\\\\infty\\$, the \\$1\\$-NN classifier is only a\n",
    "factor 2 worse than the best possible classifier. Bad news: We are\n",
    "cursed!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of Dimensionality\n",
    "\n",
    "#### Distances between points\n",
    "\n",
    "The \\$k\\$NN classifier makes the assumption that similar points share\n",
    "similar labels. Unfortunately, in high dimensional spaces, points that\n",
    "are drawn from a probability distribution, tend to never be close\n",
    "together. We can illustrate this on a simple example. We will draw\n",
    "points uniformly at random within the unit cube (illustrated in the\n",
    "figure) and we will investigate how much space the \\$k\\$ nearest\n",
    "neighbors of a test point inside this cube will take up.\n",
    "\n",
    "Formally, imagine the unit cube \\$\\[0,1\\]\\^d\\$. All training data is\n",
    "sampled *uniformly* within this cube, i.e. \\$\\\\forall i,\n",
    "x\\_i\\\\in\\[0,1\\]\\^d\\$, and we are considering the \\$k=10\\$ nearest\n",
    "neighbors of such a test point.\n",
    "\n",
    "> ![](note2_3.png)\n",
    "\n",
    "Let \\$\\\\ell\\$ be the edge length of the smallest hyper-cube that\n",
    "contains all \\$k\\$-nearest neighbor of a test point. Then\n",
    "\\$\\\\ell\\^d\\\\approx\\\\frac{k}{n}\\$ and\n",
    "\\$\\\\ell\\\\approx\\\\left(\\\\frac{k}{n}\\\\right)\\^{1/d}\\$. If \\$n= 1000\\$, how\n",
    "big is \\$\\\\ell\\$?\n",
    "\n",
    "\\$d\\$ | \\$\\\\ell\\$\n",
    "-|-\n",
    "2|0.1\n",
    "10|0.63\n",
    "100|0.955\n",
    "1000|0.9954\n",
    "\n",
    "So as \\$d\\\\gg 0\\$ almost the entire space is needed to find the\n",
    "\\$10\\$-NN. This breaks down the \\$k\\$-NN assumptions, because the\n",
    "\\$k\\$-NN are not particularly closer (and therefore more similar) than\n",
    "any other data points in the training set. Why would the test point\n",
    "share the label with those \\$k\\$-nearest neighbors, if they are not\n",
    "actually similar to it?\n",
    "\n",
    "> ![](cursefigure.png)\n",
    "> Figure demonstrating \"the curse of dimensionality\". The histogram\n",
    "> plots show the distributions of all pairwise distances between\n",
    "> randomly distributed points within \\$d\\$-dimensional unit squares. As\n",
    "> the number of dimensions \\$d\\$ grows, all distances concentrate within\n",
    "> a very small range.\n",
    "\n",
    "One might think that one rescue could be to increase the number of\n",
    "training samples, \\$n\\$, until the nearest neighbors are truly close to\n",
    "the test point. How many data points would we need such that \\$\\\\ell\\$\n",
    "becomes truly small? Fix \\$\\\\ell=\\\\frac{1}{10}=0.1\\$ \\$\\\\Rightarrow\\$\n",
    "\\$n=\\\\frac{k}{\\\\ell\\^d}=k\\\\cdot 10\\^d\\$, which grows exponentially! For\n",
    "\\$d&gt;100\\$ we would need far more data points than there are electrons\n",
    "in the universe..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distances to hyperplanes\n",
    "\n",
    "So the distance between two randomly drawn data points increases\n",
    "drastically with their dimensionality. How about the distance to a\n",
    "hyperplane? Consider the following figure. There are two blue points and\n",
    "a red hyperplane. The left plot shows the scenario in 2d and the right\n",
    "plot in 3d. As long as \\$d=2\\$, the distance between the two points is\n",
    "\\$\\\\sqrt{{\\\\Delta x}\\^2+{\\\\Delta y}\\^2}\\$. When a third dimension is\n",
    "added, this extends to \\$\\\\sqrt{{\\\\Delta x}\\^2+{\\\\Delta y}\\^2+\\\\Delta\n",
    "z\\^2}\\$, which must be at least as large (and is probably larger). This\n",
    "confirms again that pairwise distances grow in high dimensions. On the\n",
    "other hand, the distance to the red hyperplane remains unchanged as the\n",
    "third dimension is added. The reason is that the normal of the\n",
    "hyper-plane is orthogonal to the new dimension. This is a crucial\n",
    "observation. In \\$d\\$ dimensions, \\$d-1\\$ dimensions will be orthogonal\n",
    "to the normal of any given hyper-plane. Movement in those dimensions\n",
    "cannot increase or decrease the distance to the hyperplane --- the\n",
    "points just shift around and remain at the same distance. As distances\n",
    "between pairwise points become very large in high dimensional spaces,\n",
    "distances to hyperplanes become comparatively tiny. For machine learning\n",
    "algorithms, this is highly relevant. As we will see later on, many\n",
    "classifiers (e.g. the [Perceptron](lecturenote03.html) or\n",
    "[SVMs](lecturenote09.html)) place hyper planes between concentrations of\n",
    "different classes. One consequence of the curse of dimensionality is\n",
    "that most data points tend to be very close to these hyperplanes and it\n",
    "is often possible to perturb input slightly (and often imperceptibly) in\n",
    "order to change a classification outcome. This practice has recently\n",
    "become known as the creation of [adversarial\n",
    "samples](https://arxiv.org/pdf/1312.6199.pdf), whose existents is often\n",
    "falsely attributed to the complexity of neural networks.\n",
    "\n",
    "![](cursePointHyperplane.png)\n",
    "\n",
    "> The curse of dimensionality has different effects on distances between\n",
    "> two points and distances between points and hyperplanes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data with low dimensional structure\n",
    "-----------------------------------\n",
    "\n",
    "However, not all is lost. Data may lie in low dimensional subspace or on\n",
    "sub-manifolds. Example: natural images (digits, faces). Here, the true\n",
    "dimensionality of the data can be much lower than its ambient space. The\n",
    "next figure shows an example of a data set sampled from a 2-dimensional\n",
    "manifold (i.e. a surface in space), that is embedded within 3d. Human\n",
    "faces are a typical example of an intrinsically low dimensional data\n",
    "set. Although an image of a face may require 18M pixels, a person may be\n",
    "able to describe this person with less than 50 attributes (e.g.\n",
    "male/female, blond/dark hair, ...) along which faces vary.\n",
    "\n",
    "![](manifold.png)\n",
    "\n",
    "> An example of a data set in 3d that is drawn from an underlying\n",
    "> 2-dimensional manifold. The blue points are confined to the pink\n",
    "> surface area, which is embedded in a 3-dimensional ambient space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-NN summary\n",
    "\n",
    "-   \\$k\\$-NN is a simple and effective classifier if distances reliably\n",
    "    reflect a semantically meaningful notion of the dissimilarity. (It\n",
    "    becomes truly competitive through metric learning)\n",
    "-   As \\$n \\\\to \\\\infty\\$, \\$k\\$-NN becomes provably very accurate, but\n",
    "    also very slow.\n",
    "-   As \\$d \\\\gg 0\\$, points drawn from a probability distribution stop\n",
    "    being similar to each other, and the \\$k\\$NN assumption breaks down.\n",
    "\n",
    "### Reference\n",
    "\n",
    "\n",
    "[1]Cover, Thomas, and, Hart, Peter. Nearest neighbor pattern\n",
    "classification. Information Theory, IEEE Transactions on, 1967,\n",
    "13(1): 21-27\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
